#cloud-config
hostname: logstash
repo_update: true
repo_upgrade: all
apt_sources:
  - source: "deb https://artifacts.elastic.co/packages/5.x/apt stable main"
    key: |
      -----BEGIN PGP PUBLIC KEY BLOCK-----
      Version: GnuPG v2.0.14 (GNU/Linux)

      mQENBFI3HsoBCADXDtbNJnxbPqB1vDNtCsqhe49vFYsZN9IOZsZXgp7aHjh6CJBD
      A+bGFOwyhbd7at35jQjWAw1O3cfYsKAmFy+Ar3LHCMkV3oZspJACTIgCrwnkic/9
      CUliQe324qvObU2QRtP4Fl0zWcfb/S8UYzWXWIFuJqMvE9MaRY1bwUBvzoqavLGZ
      j3SF1SPO+TB5QrHkrQHBsmX+Jda6d4Ylt8/t6CvMwgQNlrlzIO9WT+YN6zS+sqHd
      1YK/aY5qhoLNhp9G/HxhcSVCkLq8SStj1ZZ1S9juBPoXV1ZWNbxFNGwOh/NYGldD
      2kmBf3YgCqeLzHahsAEpvAm8TBa7Q9W21C8vABEBAAG0RUVsYXN0aWNzZWFyY2gg
      KEVsYXN0aWNzZWFyY2ggU2lnbmluZyBLZXkpIDxkZXZfb3BzQGVsYXN0aWNzZWFy
      Y2gub3JnPokBOAQTAQIAIgUCUjceygIbAwYLCQgHAwIGFQgCCQoLBBYCAwECHgEC
      F4AACgkQ0n1mbNiOQrRzjAgAlTUQ1mgo3nK6BGXbj4XAJvuZDG0HILiUt+pPnz75
      nsf0NWhqR4yGFlmpuctgCmTD+HzYtV9fp9qW/bwVuJCNtKXk3sdzYABY+Yl0Cez/
      7C2GuGCOlbn0luCNT9BxJnh4mC9h/cKI3y5jvZ7wavwe41teqG14V+EoFSn3NPKm
      TxcDTFrV7SmVPxCBcQze00cJhprKxkuZMPPVqpBS+JfDQtzUQD/LSFfhHj9eD+Xe
      8d7sw+XvxB2aN4gnTlRzjL1nTRp0h2/IOGkqYfIG9rWmSLNlxhB2t+c0RsjdGM4/
      eRlPWylFbVMc5pmDpItrkWSnzBfkmXL3vO2X3WvwmSFiQbkBDQRSNx7KAQgA5JUl
      zcMW5/cuyZR8alSacKqhSbvoSqqbzHKcUQZmlzNMKGTABFG1yRx9r+wa/fvqP6OT
      RzRDvVS/cycws8YX7Ddum7x8uI95b9ye1/Xy5noPEm8cD+hplnpU+PBQZJ5XJ2I+
      1l9Nixx47wPGXeClLqcdn0ayd+v+Rwf3/XUJrvccG2YZUiQ4jWZkoxsA07xx7Bj+
      Lt8/FKG7sHRFvePFU0ZS6JFx9GJqjSBbHRRkam+4emW3uWgVfZxuwcUCn1ayNgRt
      KiFv9jQrg2TIWEvzYx9tywTCxc+FFMWAlbCzi+m4WD+QUWWfDQ009U/WM0ks0Kww
      EwSk/UDuToxGnKU2dQARAQABiQEfBBgBAgAJBQJSNx7KAhsMAAoJENJ9ZmzYjkK0
      c3MIAIE9hAR20mqJWLcsxLtrRs6uNF1VrpB+4n/55QU7oxA1iVBO6IFu4qgsF12J
      TavnJ5MLaETlggXY+zDef9syTPXoQctpzcaNVDmedwo1SiL03uMoblOvWpMR/Y0j
      6rm7IgrMWUDXDPvoPGjMl2q1iTeyHkMZEyUJ8SKsaHh4jV9wp9KmC8C+9CwMukL7
      vM5w8cgvJoAwsp3Fn59AxWthN3XJYcnMfStkIuWgR7U2r+a210W6vnUxU4oN0PmM
      cursYPyeV0NX/KQeUeNMwGTFB6QHS/anRaGQewijkrYYoTNtfllxIu9XYmiBERQ/
      qPDlGRlOgVTd9xUfHFkzB52c70E=
      =92oX
      -----END PGP PUBLIC KEY BLOCK-----
packages:
  - rsyslog-relp
  - openjdk-8-jre-headless
  - elasticsearch
  - logstash
  - kibana
write_files:
  - path: /etc/rsyslog.d/39-logstash.conf
    content: |
      $ModLoad omrelp
      *.* :omrelp:127.0.0.1:2514
  - path: /etc/elasticsearch/elasticsearch.opts
    content: |
      -Xms350m
      -Xmx350m
      -XX:+UseConcMarkSweepGC
      -XX:CMSInitiatingOccupancyFraction=75
      -XX:+UseCMSInitiatingOccupancyOnly
      -XX:+DisableExplicitGC
      -XX:+AlwaysPreTouch
      -server
      -Xss1m
      -Djava.awt.headless=true
      -Dfile.encoding=UTF-8
      -Djna.nosys=true
      -Djdk.io.permissionsUseCanonicalPath=true
      -Dio.netty.noUnsafe=true
      -Dio.netty.noKeySetOptimization=true
      -Dio.netty.recycler.maxCapacityPerThread=0
      -Dlog4j.shutdownHookEnabled=false
      -Dlog4j2.disable.jmx=true
      -Dlog4j.skipJansi=true
      -XX:+HeapDumpOnOutOfMemoryError
  - path: /etc/kibana/kibana.yml
    content: |
      server.host: 0.0.0.0
  - path: /etc/logstash/jvm.options
    content: |
      -Xmx350m
      -Xms350m
      -XX:+UseParNewGC
      -XX:+UseConcMarkSweepGC
      -XX:CMSInitiatingOccupancyFraction=75
      -XX:+UseCMSInitiatingOccupancyOnly
      -XX:+DisableExplicitGC
      -Djava.awt.headless=true
      -Dfile.encoding=UTF-8
      -XX:+HeapDumpOnOutOfMemoryError
  - path: /etc/logstash/patterns/elb
    content: |
      ELB_LINE %{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:elb} %{IP:clientip}:%{INT:clientport:int} (?:(%{IP:backendip}:?:%{INT:backendport:int})|-) %{NUMBER:request_processing_time:float} %{NUMBER:backend_processing_time:float} %{NUMBER:response_processing_time:float} (?:-|%{INT:elb_status_code:int}) (?:-|%{INT:backend_status_code:int}) %{INT:received_bytes:int} %{INT:sent_bytes:int} \"%{ELB_REQUEST_LINE}\" \"(?:-|%{DATA:user_agent})\" (?:-|%{NOTSPACE:ssl_cipher}) (?:-|%{NOTSPACE:ssl_protocol})
  - path: /etc/logstash/patterns/worker
    content: |
      WORKER_BASE %{LOGLEVEL:level} \s*%{NOTSPACE:class} -

      WORKER_FILE_CONVERSION %{WORKER_BASE} Converting %{GREEDYDATA:filename} \(%{UUID:uuid}, %{INT:size:int}kb\) to PDF, time: %{INT:duration:int}ms

      WORKER_DOCSET_CREATION %{WORKER_BASE} Created DocumentSet %{INT:documentSetId}. cluster %{INT:clusterDuration:int}ms; total %{INT:totalDuration:int}ms
  - path: /etc/logstash/patterns/web
    content: |
      WEB_BASE %{LOGLEVEL:level} \s*%{NOTSPACE:class} - \s*

      WEB_REQUEST %{WEB_BASE} %{INT:duration}ms %{NOTSPACE:method} %{URIPATH:path}(?:%{URIPARAM:query_string})? -> %{INT:http_status_code}
  - path: /etc/logstash/conf.d/logstash.conf
    content: |
      input {
        relp {
          port => 2514
          type => relp
        }

        s3 {
          bucket => "overview-production-load-balancer-logs"
          delete => true
          type => elb
        }
      }

      filter {
        if [type] == "relp" {
          # Remove syslog stuff right away
          grok {
            patterns_dir => [ "/etc/logstash/patterns" ]
            match => [ "message", "%{SYSLOGLINE}" ]
            add_tag => "syslog"
            overwrite => [ "message" ]
            break_on_match => false
          }
          syslog_pri {}
        }

        if [program] == "postgres" {
          # Stitch together logged SQL queries. Each line starts with [\d+-1], so
          # subsequent lines start with [\d+-(not 1)].
          multiline {
            pattern => "^\[\d+-(?:1\d|[2-9])\d*\]"
            what => "previous"
          }
        } else if [program] == "run-worker.sh" or [program] == "run-web.sh" or [program] == "java" {
          # This is one of our Java programs

          # Stitch together exceptions
          multiline {
            pattern => "^\s*(at |Caused by:)"
            what => "previous"
          }

          # HTTP request
          grok {
            patterns_dir => [ "/etc/logstash/patterns" ]
            match => [ "message", "%{WEB_REQUEST}" ]
            add_tag => [ "web", "http-request" ]
            remove_tag => [ "_grokparsefailure" ]
          }

          # File conversion
          grok {
            patterns_dir => [ "/etc/logstash/patterns" ]
            match => [ "message", "%{WORKER_FILE_CONVERSION}" ]
            add_tag => [ "worker", "file-conversion" ]
            remove_tag => [ "_grokparsefailure" ]
          }
        
          # Docset creation
          grok {
            patterns_dir => [ "/etc/logstash/patterns" ]
            match => [ "message", "%{WORKER_DOCSET_CREATION}" ]
            add_tag => [ "worker", "docset-creation" ]
            remove_tag => [ "_grokparsefailure" ]
          }
        
          # Stack trace
          grok {
            patterns_dir => [ "/etc/logstash/patterns" ]
            match => [ "message", "%{JAVASTACKTRACEPART}" ]
            add_tag => [ "notify", "java-exception" ]
            remove_tag => [ "_grokparsefailure" ]
          }
        }
      }

      output {
        elasticsearch { }

        if ("notify" in [tags]) and !([class] == "org.jboss.netty.channel.socket.nio.AbstractNioWorker" and [message] == "cleanUpWriteBuffer") {
          sns {
            arn => "arn:aws:sns:us-east-1:711916283702:LogstashAlarms"
            region => "us-east-1"
          }
        }
      }
runcmd:
  - echo "127.0.1.1 $(hostname)" | sudo tee -a /etc/hosts
  - /usr/share/logstash/bin/logstash-plugin install logstash-input-relp logstash-input-s3 logstash-filter-multiline logstash-filter-syslog_pri logstash-output-sns
  - systemctl daemon-reload
  - systemctl restart rsyslog
  - systemctl enable elasticsearch.service
  - systemctl restart elasticsearch.service
  - systemctl enable logstash.service
  - systemctl restart logstash.service
  - systemctl enable kibana.service
  - systemctl restart kibana.service
